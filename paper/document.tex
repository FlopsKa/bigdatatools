\documentclass[a4paper,11pt]{scrartcl}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\begin{document}

\title{Analyzing Big Data Streams}
\author{Florian Kalinke%
   \thanks{E-mail: \texttt{flops.ka@gmail.com}}}
\date{November 2017}
\maketitle

\begin{abstract}
   Das ist die Kurzfassung.
\end{abstract}

\section{Einleitung}
Der Begriff \textit{Big Data} ist eines der aktuellen Buzzwords der Informatik.
Unternehmen speichern ihre Daten ab und versuchen Erkenntnisse aus dem
Datenbestand abzuleiten. Abhängig von diesen Ergebnissen können strategische
Entscheidungen getroffen werden, um dem Unternehmen so einen wirtschaftlichen
Vorteil zu ermöglichen. Historisch gesehen ist dieses Vorgehen bereits
etabliert - die zu analysierende Datenmenge steigt allerdings stark an und
schafft so neue Herausforderungen bei der Analyse der Daten.

Obwohl nicht genau definiert ist, ab wann es sich bei der Datenverarbeitung um
\textit{Big Data} handelt, hat sich die folgende Definition durchgesetzt:

Datenmengen, die zu groß, zu komplex oder zu schnelllebig sind, um sie mit
traditionellen Methoden der Datenhaltung zu speichern und auszuwerten werden
als Big Data bezeichnet. Diese Einordnung geht davon aus, dass die Daten einem
oder mehreren der 3 „V“s entsprechen:
\begin{description}
  \item[Volume] Die Datenmengen sind im Tera-, Peta- oder Exabytebereich.
  \item[Velocity] Die Daten müssen in Echtzeit verarbeitet und analysiert
    werden.
  \item[Variety] Die Daten müssen keinem bestimmten Schema entsprechen, sie
    sind sowohl strukturiert, semistrukturiert als auch unstrukturiert.
\end{description}

Bei der Verarbeitung der Daten wird zwischen der \textit{Batch-} und
\textit{Stream-}Verarbeitung differenziert. Die Batch-Verarbeitung geht davon
aus, dass auf einer begrenzten Menge von Daten operiert wird. Das bedeutet,
dass die Größe der Daten bekannt und endlich ist. Die Datenmenge kann
„vollständig“ verarbeitet werden. Im Gegensatz dazu betrachtet die
Stream-Verarbeitung die Verarbeitung von Daten, die erst während eines
zeitlichen Verlaufs verfügbar werden.

Betrachtet man exemplarisch die Analyse der Besucherzahlen einer Webseite, so
lassen sich über Batch-Verarbeitung beispielsweise die Fragen beantworten „Wie
viele Personen haben die Webseite gestern aufgerufen? Wie viele vorgestern?
Wie viele in der vergangenen Woche?

Analog betrachtet die Stream-Verarbeitung die Frage: Wie viele Besucher waren
in der letzten Minute auf der Seite aktiv? Wie viele in den letzten 10
Sekunden? Sind aktuell Besucher auf der Seite? Über die Stream-Verarbeitung
können diese Daten in Echzeit analysiert und die Ergebnisse betrachtet werden.

\section{Big Data}
\section{CAP-Theorem}
\section{Lambda Architektur}
\subsection{Batch Layer}
\subsection{Speed / Realtime Layer}

\section{Hadoop Kernkomponenten}
\subsection{Hadoop Filesystem}
\subsection{MapReduce Framework}

\section{Verarbeitung von Streams}

\section{Echtzeitanalyse von Tastatureingaben}

\section{Zusammenfassung / Fazit}


\end{document}
