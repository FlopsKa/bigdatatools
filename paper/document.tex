\documentclass[a4paper,11pt]{scrartcl}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\begin{document}

\title{Analyzing Big Data Streams}
\author{Florian Kalinke%
   \thanks{E-mail: \texttt{flops.ka@gmail.com}}}
\date{November 2017}
\maketitle

\begin{abstract}
   Das ist die Kurzfassung.
\end{abstract}

\section{Einleitung}
Der Begriff \textit{Big Data} ist eines der aktuellen Buzzwords der Informatik.
Unternehmen speichern ihre Daten ab und versuchen Erkenntnisse aus dem
Datenbestand abzuleiten. Abhängig von diesen Ergebnissen können strategische
Entscheidungen getroffen werden, um dem Unternehmen so einen wirtschaftlichen
Vorteil zu ermöglichen. Historisch gesehen ist dieses Vorgehen bereits
etabliert - die zu analysierende Datenmenge steigt allerdings stark an und
schafft so neue Herausforderungen bei der Analyse der Daten.

Obwohl nicht genau definiert ist, ab wann es sich bei der Datenverarbeitung um
\textit{Big Data} handelt, hat sich die folgende Definition durchgesetzt:

Datenmengen, die zu groß, zu komplex oder zu schnelllebig sind, um sie mit
traditionellen Methoden der Datenhaltung zu speichern und auszuwerten werden
als Big Data bezeichnet. Diese Einordnung geht davon aus, dass die Daten einem
oder mehreren der 3 „V“s entsprechen:
\begin{description}
  \item[Volume] Die Datenmengen sind im Tera-, Peta- oder Exabytebereich.
  \item[Velocity] Die Daten müssen in Echtzeit verarbeitet und analysiert
    werden.
  \item[Variety] Die Daten müssen keinem bestimmten Schema entsprechen, sie
    sind sowohl strukturiert, semistrukturiert als auch unstrukturiert.
\end{description}



\section{Big Data}
\section{CAP-Theorem}
\section{Lambda Architektur}
\subsection{Batch Layer}
\subsection{Speed / Realtime Layer}

\section{Hadoop Kernkomponenten}
\subsection{Hadoop Filesystem}
\subsection{MapReduce Framework}

\section{Verarbeitung von Streams}

\section{Echtzeitanalyse von Tastatureingaben}

\section{Zusammenfassung / Fazit}


\end{document}
