\documentclass[a4paper,11pt]{scrartcl}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\begin{document}

\title{Analyzing Big Data Streams}
\author{Florian Kalinke%
   \thanks{E-mail: \texttt{flops.ka@gmail.com}}}
\date{November 2017}
\maketitle

\begin{abstract}
   Das ist die Kurzfassung.
\end{abstract}

\section{Einleitung}
Der Begriff \textit{Big Data} ist eines der aktuellen Buzzwords der Informatik.
Unternehmen speichern ihre Daten ab und versuchen Erkenntnisse aus dem
Datenbestand abzuleiten. Abhängig von diesen Ergebnissen können strategische
Entscheidungen getroffen werden, um dem Unternehmen so einen wirtschaftlichen
Vorteil zu ermöglichen. Historisch gesehen ist dieses Vorgehen bereits
etabliert - die zu analysierende Datenmenge steigt allerdings stark an und
schafft so neue Herausforderungen bei der Analyse der Daten.

Obwohl nicht genau definiert ist, ab wann es sich bei der Datenverarbeitung um
\textit{Big Data} handelt, hat sich die folgende Definition durchgesetzt:

Datenmengen, die zu groß, zu komplex oder zu schnelllebig sind, um sie mit
traditionellen Methoden der Datenhaltung zu speichern und auszuwerten werden
als Big Data bezeichnet. Diese Einordnung geht davon aus, dass die Daten einem
oder mehreren der 3 „V“s entsprechen:
\begin{description}
  \item[Volume] Die Datenmengen sind im Tera-, Peta- oder Exabytebereich.
  \item[Velocity] Die Daten müssen in Echtzeit verarbeitet und analysiert
    werden.
  \item[Variety] Die Daten müssen keinem bestimmten Schema entsprechen, sie
    sind sowohl strukturiert, semistrukturiert als auch unstrukturiert.
\end{description}

Bei der Verarbeitung der Daten wird zwischen der \textit{Batch-} und
\textit{Stream-}Verarbeitung differenziert. Die Batch-Verarbeitung geht davon
aus, dass auf einer begrenzten Menge von Daten operiert wird. Das bedeutet,
dass die Größe der Daten bekannt und endlich ist. Die Datenmenge kann
„vollständig“ verarbeitet werden. Im Gegensatz dazu betrachtet die
Stream-Verarbeitung die Verarbeitung von Daten, die erst während eines
zeitlichen Verlaufs verfügbar werden.

Betrachtet man exemplarisch die Analyse der Besucherzahlen einer Webseite, so
lassen sich über Batch-Verarbeitung beispielsweise die Fragen beantworten „Wie
viele Personen haben die Webseite gestern aufgerufen? Wie viele vorgestern?
Wie viele in der vergangenen Woche?

Analog betrachtet die Stream-Verarbeitung die Fragestellungen: Wie viele
Besucher waren in der letzten Minute auf der Seite aktiv? Wie viele in den
letzten 10 Sekunden? Sind aktuell Besucher auf der Seite? Über
Stream-Verarbeitung können diese Daten in Echzeit analysiert und die Ergebnisse
betrachtet werden.

Die vorliegende Arbeit gibt einen allgemeinen Einblick in die Verarbeitung von
Big Data mit dem Hadoop Framework und zeigt im Anschluss detailliert die
Möglichkeiten der Verarbeitung von Streams.

% TODO Kapitelbeschreibung

\section{Datenhaltung bei Big Data}
Die Verarbeitung von den in der Einleitung beschriebenen Datenmengen bringt
eine Menge an Herausforderungen mit sich: Ein einzelner Rechner ist zu schwach,
um die Menge an Daten a) abzuspeichern und b) Berechnungen auf diesen Daten
auszuführen. Aus diesem Grund wird für die Verarbeitung von Big Data im
Normalfall \textit{horizontal} skaliert. Das bedeutet, dass die Verarbeitung
statt auf einem extrem performanten Rechner auf viele Rechner verteilt wird.
Hier kommt sogenannte \textit{Commodity Hardware} zum Einsatz: Handelsübliche
Rechner, die im Verbund agieren und auf die anfallende Arbeit aufgeteilt wird.

Reduziert man Big Data auf die zu speichernde Datenmenge gibt es drei
Hauptgründe, die Last auf verschiedene Rechner zu verteilen:

\begin{description}
  \item[Skalierbarkeit] Die Daten können aufgrund des Speicherplatzes, der
    Schreibgeschwindigkeit oder der Lesegeschwindigkeit nicht auf einem
    einzelnen Rechner gespeichert werden.
  \item[Fehlertoleranz / Ausfallsicherheit] Das System soll nicht von einer
    einzelnen Maschine abhängig sein. Fällt ein Rechner aus, kann ein anderer
    die Arbeit übernehmen. Je mehr Rechner an einem System beteiligt sind,
    desto wahrscheinlicher ist es, dass einzelne Rechner oder einzelne
    Komponenten ausfallen.
  \item[Latenz] Sind die Daten auf einem Rechner, der sich geographisch nah am
    Ort des Zugriffs befindet, so müssen Netwerkpakete eine geringere Strecke
    zurücklegen und der Zugriff wird beschleunigt.
\end{description}

Um Daten verteilt vorzuhalten, gibt es zwei Ansätze:

\begin{description}
  \item[Replikation] Die gleichen Daten werden auf verschiedenen Knoten
    vorgehalten, sind also redundant gespeichert. Fällt ein Knoten aus, so
    können die Daten von einem anderen Knoten gelesen werden.
  \item[Partitionierung] Eine große Datenbank wird in Teilmengen zerlegt, die
    dann verschiedenen Knoten zugewiesen werden.
\end{description}

Die beiden Ansätze werden in der Praxis häufig kombiniert eingesetzt.

\section{Das CAP-Theorem}
Das CAP-Theorem oder \textit{Brewer's Theorem} wurde öffentlich erstmalig im
Jahr 2000 auf dem \textit{Symposium on Principles of Distributed Computing}
vorgestellt und im Jahr 2002 formal bewiesen. Das Theorem besagt, dass ein
verteiltes System nicht mehr als zwei der folgenden drei Garantien bieten kann:

\begin{description}
  \item[Konsistenz] Das Lesen eines Wertes liefert immer das Ergebnis des
    letzten Schreibens.
  \item[Verfügbarkeit] Das Lesen eines Wertes liefert immer ein Ergebnis,
    allerdings ohne die Garantie, dass der letzte Schreibvorgang berücksichtigt
    wurde.
  \item[Partitionstoleranz] Das System funktioniert unabhängig von der Zahl der
    verlorenen oder verzögerten Netzwerkpakete.
\end{description}

Graphisch wird das CAP-Theorem häufig als Pyramide dargestellt:

% TODO Graphik vom CAP-Theorem

\section{Lambda Architektur}
Die Lambda-Architektur wurde 2011 von Nathan Marz im Blogpost „How to beat the
CAP-Theorem“ vorgestellt und beruht auf der Beobachtung, dass sich die
Komplexität in verteilten Systemen aus dem veränderlichen Zustand in
Datenbanken („mutable state“) und der Nutzung von inkrementellen Algorithmen
zur Manipulation dieses Zustands ergibt.

Der Autor wirft die Frage auf, wie ein System aussehen würde, dass als
Kernkomponente eine Append-Only-Datenbank verwendet - das heißt der Zustand
eines Datums in der Datenbank kann nicht verändert werden. Berechnungen werden
auf sämtlichen vorhandenen Daten ausgeführt und die Nutzung von inkrementellen
Algorithmen entfällt.

$\text{Query} = \text{Function(All Data)}$

Die Abfrage wird als Batch-Verarbeitung ausgeführt und das Ergebnis
abgespeichert. Eine Berechnung auf sämtlichen vorhandenen Daten kann sehr lange
dauern, dass heißt es gibt keine aktuelle Sicht auf die Daten.

Das Problem wird dadurch gelöst, dass historische Daten über einen Batch-Job
verarbeitet werden. Neu anfallende Daten verarbeitet der sogenannte Speed- bzw.
Realtime Layer und dieser füllt so die entstehende Lücke auf.

\subsection{Batch Layer}
\subsection{Speed / Realtime Layer}

\section{Hadoop Kernkomponenten}
\subsection{Hadoop Filesystem}
\subsection{MapReduce Framework}

\section{Verarbeitung von Streams}

\section{Echtzeitanalyse von Tastatureingaben}

\section{Zusammenfassung / Fazit}


\end{document}
